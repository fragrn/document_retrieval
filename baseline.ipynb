{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    ''' 读取 json 文件 '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(data, path):\n",
    "    ''' 写入 json 文件 '''\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return torch.nn.functional.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_documents(query_embedding, document_embeddings, query_token_list, document_token_lists, k=3, alpha=0.3):\n",
    "    \"\"\"\n",
    "    从所有document embeddings中检索出与query embedding最相关的前k个document。\n",
    "    \"\"\"\n",
    "    embedding_similarities = cosine_similarity(query_embedding, document_embeddings) #26599*1\n",
    "    token_id_similarities = [] #26599*1\n",
    "    for document_token_list in document_token_lists:\n",
    "        token_id_similarities.append(jaccard_similarity(query_token_list, document_token_list))\n",
    "    \n",
    "    token_id_similarities = torch.tensor(token_id_similarities, device=device)\n",
    "    combined_similarities = alpha * embedding_similarities + (1 - alpha) * token_id_similarities\n",
    "    _, top_document_indices = combined_similarities.topk(k)\n",
    "    return top_document_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(top_document_indices,query_embedding, document_embeddings,query_token_list,document_token_lists, k=3):\n",
    "    \"\"\"\n",
    "    根据fact_input_list和query_input_list之间的相似度对初始检索结果进行重新排序。\n",
    "    \"\"\"\n",
    "    new_scores = []\n",
    "    token_id_similarities = [] #26599*1\n",
    "    embedding_similarities = [] #26599*1\n",
    "\n",
    "    for idx in top_document_indices[0]:\n",
    "#         embedding_similarities.append(torch.nn.functional.cosine_similarity(query_embedding, document_embeddings[idx]))\n",
    "        token_id_similarities.append(jaccard_similarity(query_token_list, document_token_lists[idx]))\n",
    "    \n",
    "    token_id_similarities = torch.tensor(token_id_similarities, device=device)\n",
    "    combined_similarities = alpha * embedding_similarities + (1 - alpha) * token_id_similarities\n",
    "    _, top_document_indices = token_id_similarities.topk(k)\n",
    "    return top_document_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_fun():\n",
    "    path=os.getcwd()\n",
    "    newpath=path+\"/output/\"\n",
    "    os.chdir(newpath)\n",
    "    os.system('zip prediction.zip result.json')\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取query_testset文件（512条）  512*74\n",
    "query = read_json('input/query_testset.json')\n",
    "# query_embeddings = torch.tensor([entry['query_embedding'] for entry in query], device=device)\n",
    "query_token_lists = [entry['query_input_list'] for entry in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取检索fact（26599条）     26599*90\n",
    "document = read_json('input/document.json')\n",
    "document_embeddings = torch.tensor([entry['facts_embedding'] for entry in document], device=device)\n",
    "document_token_lists = [entry['fact_input_list'] for entry in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:50<00:00,  4.64it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for item in tqdm.tqdm(query):\n",
    "    result = {}\n",
    "    query_embedding = torch.tensor(item['query_embedding'], device=device)\n",
    "    query_token_list=item['query_input_list']\n",
    "    top_document_indices = retrieve_top_k_documents(query_embedding, document_embeddings,query_token_list,document_token_lists, k=3,alpha=0.3)\n",
    "    reranked_indices = rerank_documents(top_document_indices,query_embedding, document_embeddings,query_token_list,document_token_lists, k=3)\n",
    "    result['query_input_list'] = item['query_input_list']\n",
    "    result['evidence_list'] = [{'fact_input_list': document[index]['fact_input_list']} for index in top_document_indices[0]]\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(results, 'output/result.json')\n",
    "print('write to output/result.json successful')\n",
    "zip_fun()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

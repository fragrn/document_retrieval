{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    ''' 读取 json 文件 '''\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(data, path):\n",
    "    ''' 写入 json 文件 '''\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(list1, list2):\n",
    "    set1, set2 = set(list1), set(list2)\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return torch.nn.functional.cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k_documents(query_embedding, document_embeddings, query_token_list, document_token_lists, k=3, alpha=0.5):\n",
    "    \"\"\"\n",
    "    从所有document embeddings中检索出与query embedding最相关的前k个document。\n",
    "    \"\"\"\n",
    "    embedding_similarities = cosine_similarity(query_embedding, document_embeddings) #26599*1\n",
    "    token_id_similarities = [] #26599*1\n",
    "    for document_token_list in document_token_lists:\n",
    "        token_id_similarities.append(jaccard_similarity(query_token_list, document_token_list))\n",
    "    \n",
    "    token_id_similarities = torch.tensor(token_id_similarities, device=device)\n",
    "    combined_similarities = alpha * embedding_similarities + (1 - alpha) * token_id_similarities\n",
    "    _, top_document_indices = combined_similarities.topk(k)\n",
    "    # print(f\"firstrank:{top_document_indices}\")\n",
    "    return top_document_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_documents(top_document_indices,query_embedding, document_embeddings,query_token_list,document_token_lists, k=3,alpha=0.1):\n",
    "    \"\"\"\n",
    "    根据fact_input_list和query_input_list之间的相似度对初始检索结果进行重新排序。\n",
    "    \"\"\"\n",
    "    token_id_similarities = []  # 用于存储Jaccard相似度\n",
    "    embedding_similarities = []  # 用于存储余弦相似度\n",
    "    \n",
    "    # 确保query_embedding是2D张量\n",
    "    query_embedding = query_embedding.unsqueeze(0)\n",
    "    \n",
    "    for idx in top_document_indices[0]:\n",
    "        # 计算余弦相似度\n",
    "        cosine_sim = torch.nn.functional.cosine_similarity(query_embedding, document_embeddings[idx].unsqueeze(0), dim=1)\n",
    "        embedding_similarities.append(cosine_sim.item())\n",
    "        \n",
    "        # 计算Jaccard相似度\n",
    "        token_id_similarities.append(jaccard_similarity(query_token_list, document_token_lists[idx]))\n",
    "    \n",
    "    # 转换为tensor\n",
    "    embedding_similarities = torch.tensor(embedding_similarities, device=device)\n",
    "    token_id_similarities = torch.tensor(token_id_similarities, device=device)\n",
    "    combined_similarities = alpha * embedding_similarities + (1 - alpha) * token_id_similarities\n",
    "    _, top_indices_rerank = combined_similarities.topk(k)\n",
    "    result=[top_document_indices[0][i] for i in top_indices_rerank.tolist()]\n",
    "    # print(f\"rerank:{result}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_fun():\n",
    "    path=os.getcwd()\n",
    "    newpath=path+\"/output/\"\n",
    "    os.chdir(newpath)\n",
    "    os.system('zip prediction.zip result.json')\n",
    "    os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取query_testset文件（512条）  512*74\n",
    "query = read_json('input/query_testset.json')\n",
    "# query_embeddings = torch.tensor([entry['query_embedding'] for entry in query], device=device)\n",
    "query_token_lists = [entry['query_input_list'] for entry in query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取检索fact（26599条）     26599*90\n",
    "document = read_json('input/document.json')\n",
    "document_embeddings = torch.tensor([entry['facts_embedding'] for entry in document], device=device)\n",
    "document_token_lists = [entry['fact_input_list'] for entry in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:53<00:00,  4.53it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for item in tqdm.tqdm(query):\n",
    "    result = {}\n",
    "    query_embedding = torch.tensor(item['query_embedding'], device=device)\n",
    "    query_token_list=item['query_input_list']\n",
    "    top_document_indices = retrieve_top_k_documents(query_embedding, document_embeddings,query_token_list,document_token_lists, k=3,alpha=0.5)\n",
    "    # reranked_indices = rerank_documents(top_document_indices,query_embedding, document_embeddings,query_token_list,document_token_lists, k=3,alpha=0.5)\n",
    "    result['query_input_list'] = item['query_input_list']\n",
    "    result['evidence_list'] = [{'fact_input_list': document[index]['fact_input_list']} for index in top_document_indices[0]]\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write to output/result.json successful\n",
      "updating: result.json (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "write_json(results, 'output/result.json')\n",
    "print('write to output/result.json successful')\n",
    "zip_fun()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
